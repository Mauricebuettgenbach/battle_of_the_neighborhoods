{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd0119bfaff21b32781f6d2764777c04d49502b53816ae403fe48dcf9da50cdb242",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "119bfaff21b32781f6d2764777c04d49502b53816ae403fe48dcf9da50cdb242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IBM APPLIED DATA SCIENCE CAPSTONE - THE BATTLE OF NEIGHBORHOODS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TABLE OF CONTENT\n",
    "+ **THE BUSINESS CASE**\n",
    "+ **THE DATA**\n",
    "+ **THE LIBRARIES**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## THE BUSINESS CASE \n",
    "My fiance and I love pizza. We eat it minimum once a week and we are pretty picky about it. The best Pizza place in our neighbourhood is called [L'Osteria](https://losteria.net/en/). Pizzas are huge, drinks are decent, atmosphere is great and prices are reasonable - perfect place for young people like us!\n",
    "<br>\n",
    "<br>\n",
    "Your probably now wonder what this has to do with the Capstone Project for the [IBM - Data Science Professional Certficiate](https://www.coursera.org/professional-certificates/ibm-data-science). L'Osteria is a restaurant chain for italian food, founded in 1999 in Germany. The chain currently has 139 restaurants in Europe, of which the majority is located in Germany. As the chain is looking to expand further, it continuously needs to scout new locations for their next restaurants. As that can be an extremely difficult and long process, this Capstone Project aims to build an analysis tool that recommends neighbourhoods which could be a good fit for new L'Osteria locations. In result, the process of scouting new neihbourhoods should be more time and cost efficient.\n",
    "<br>\n",
    "<br>\n",
    "The tool is going to be based on a k-means clustering algorithm that uses data from Foursquare, Facebook and governmental websites. As my ressources for this project are limited, the tool will be focused on Germany only. However, the approach could be adapted to any other country or region, if enough ressources are provided. Further ressources would also enable adding more socio-economic data which would make the algorithm more precise. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## THE DATA \n",
    "As already mentioned above, we are going to use data from different sources. In this section, I will provide you with a detailed description for each data source used and provide you the code you need to download/upload the data.\n",
    "<br>\n",
    "<br>\n",
    "### FOURSQUARE DATA\n",
    "[Foursquare](https://foursquare.com/) is an app that lets its users rank all different kinds of venues all over the world. By doing so,Foursquare has build one of the most comprehensive geospatial data sets in the world. The data set covers an extremely large amount of venue and user data that allows you to access everything from a venue's name, location or menu up to user ratings, comments or pictures of each venue. And the best part: Foursquare provides developers an [API](https://developer.foursquare.com/) that is (in limits) free to use! \n",
    "<br>\n",
    "<br>\n",
    "Even though Foursquare provides an incredible variety of data, we will use the venue categories (e.g. sushi restaurant, park, sports club, italian restaurant, etc.) for our analysis only. While it would most likely increase the success of our algorithm to include more information such as rankings or types of user data, we will not include this data due to ressource limitations.\n",
    "<br>\n",
    "<br>\n",
    "To being able to access the Foursquare data during our analysis, you must [sign up](https://developer.foursquare.com/) for a developer account with Foursquare first. Afterwards you need to log into your account and navigate to \"My Apps\". Here, you can create yourself your own client ID and secret. Save these credentials as txt files and name them according to the code below.\n",
    "<br>\n",
    "Once you have created your credentials, you need to create an access token. For that, just follow [this guide](https://developer.foursquare.com/docs/places-api/authentication/) and save the access token in a txt file with the name provided below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Foursquare credentials loaded.\n"
     ]
    }
   ],
   "source": [
    "# Import working library\n",
    "import pandas as pd\n",
    "\n",
    "# Set credentials\n",
    "client_id = list(pd.read_csv('FOURSQUARE_CLIENT_ID.txt').columns)[0]\n",
    "client_secret = list(pd.read_csv('FOURSQUARE_CLIENT_SECRET.txt').columns)[0]\n",
    "access_token = list(pd.read_csv('FOURSQUARE_ACCESS_TOKEN.txt').columns)[0]\n",
    "\n",
    "# Check out\n",
    "print(\"Foursquare credentials loaded.\")"
   ]
  },
  {
   "source": [
    "You are now set to use the Foursquare API! We will use these credentials at a later point. For now, we continue with our next data source."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### FACEBOOK POPULATION DATA\n",
    "Of all the bad Corona did to us, it also had a good side effect: The big data miners of the world opened up and shared some of their digital gold with the rest of us. So did Facebook by publishing large datasets based on their app usage. You can find a detailed description of their \"Data for Good\" project under this [link](https://dataforgood.fb.com/). Most of the project's data is publicly accessible via [The Humanitarian Data Exchange](https://data.humdata.org/organization/facebook) or [AWS](https://registry.opendata.aws/dataforgood-fb-hrsl/).\n",
    "<br>\n",
    "<br>\n",
    "One of the datasets provided by Facebook contains population density information. According to Facebook, it is the world's most accurate population dataset. It is divided by countries, each containing seven datasets for the distribution of various populations in relation to their respective coordinates: \n",
    "<br>\n",
    "<br>\n",
    "1. Overall population density \n",
    "<br>\n",
    "2. Women\n",
    "<br>\n",
    "3. Men\n",
    "<br> \n",
    "4. Children (ages 0-5) \n",
    "<br>\n",
    "5. Youth (ages 15-24) \n",
    "<br>\n",
    "6. Elderly (ages 60+) \n",
    "<br>\n",
    "7. Women of reproductive age (ages 15-49)\n",
    "<br>\n",
    "<br>\n",
    "Due to limited computing ressources, we will focus on the German datasets for women and men only. You can download them under the following links. Alternatively, you can use the files in my Github repository.\n",
    "<br>\n",
    "<br>\n",
    "1. [DEU_men.csv.zip](https://data.humdata.org/dataset/7d08e2b0-b43b-43fd-a6a6-a308f222cdb2/resource/9b14d65e-ebe1-4509-8680-1f21becc75d9/download/deu_men_2019-08-03_csv.zip)\n",
    "<br>\n",
    "2. [DEU_women.csv.zip](https://data.humdata.org/dataset/7d08e2b0-b43b-43fd-a6a6-a308f222cdb2/resource/26b8ec5b-e40b-40af-a4fc-fcd85bfb0818/download/deu_women_2019-08-03_csv.zip)\n",
    "<br>\n",
    "<br>\n",
    "Once downloaded, we can execute the code below. It will load the data, transform and standardize it. Furthermore, we are merging the two datasets to receive one comprehensive population dataset. Finally, we will add a total population column to our dataset which is the sum of all male and female population. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32433580, 5)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    LATITUDE  LONGITUDE  MALE_POPULATION  FEMALE_POPULATION  TOTAL_POPULATION\n",
       "0  48.814306   8.459306         1.451906           1.552587          3.004493\n",
       "1  48.815694   8.480972         1.451906           1.552587          3.004493\n",
       "2  48.804861   8.445139         1.451906           1.552587          3.004493\n",
       "3  48.787361   8.449306         1.451906           1.552587          3.004493\n",
       "4  48.819028   8.464306         1.451906           1.552587          3.004493\n",
       "5  48.824583   8.415139         1.451906           1.552587          3.004493\n",
       "6  48.775417   8.455972         1.451906           1.552587          3.004493\n",
       "7  48.817639   8.482917         1.451906           1.552587          3.004493\n",
       "8  48.769306   8.432917         1.451906           1.552587          3.004493\n",
       "9  48.817361   8.485417         1.451906           1.552587          3.004493"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LATITUDE</th>\n      <th>LONGITUDE</th>\n      <th>MALE_POPULATION</th>\n      <th>FEMALE_POPULATION</th>\n      <th>TOTAL_POPULATION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>48.814306</td>\n      <td>8.459306</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>48.815694</td>\n      <td>8.480972</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>48.804861</td>\n      <td>8.445139</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48.787361</td>\n      <td>8.449306</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>48.819028</td>\n      <td>8.464306</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>48.824583</td>\n      <td>8.415139</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>48.775417</td>\n      <td>8.455972</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>48.817639</td>\n      <td>8.482917</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>48.769306</td>\n      <td>8.432917</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>48.817361</td>\n      <td>8.485417</td>\n      <td>1.451906</td>\n      <td>1.552587</td>\n      <td>3.004493</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Load density data\n",
    "male_population = pd.read_csv('DEU_men_2019-08-03.csv')\n",
    "female_population = pd.read_csv('DEU_women_2019-08-03.csv')\n",
    "\n",
    "# Standardize columns\n",
    "male_population.rename(columns={\"latitude\":\"LATITUDE\", \"longitude\":\"LONGITUDE\", \"population\":\"MALE_POPULATION\"}, inplace=True)\n",
    "female_population.rename(columns={\"latitude\":\"LATITUDE\", \"longitude\":\"LONGITUDE\", \"population\":\"FEMALE_POPULATION\"}, inplace=True)\n",
    "\n",
    "# Merge on geolocation\n",
    "population = male_population.merge(female_population, how='outer', on=['LATITUDE', 'LONGITUDE'])\n",
    "\n",
    "# Calculate total population column\n",
    "population['TOTAL_POPULATION'] = population['MALE_POPULATION'] + population['FEMALE_POPULATION']\n",
    "\n",
    "# Visualize\n",
    "print(population.shape)\n",
    "population.head(10)"
   ]
  },
  {
   "source": [
    "### GERMAN BORDER DATA\n",
    "As our Facebook data compromises over 32 million unique geospatial data points that are randomized, there is no way for us to filter this data according to cities or neighborhoods in Germany. One solution to this problem would be to reverse engineer the coordinates with libraries like geopy. However, the accessible APIs via geopy only allow a limited number of calls per day. For +32 million datapoints it would require month do download all necessary data this way. Of course there is always the option to upgrade to a paid enterprise plan to speed up this process. However, this would require financial ressources that I currently do not have. \n",
    "<br>\n",
    "<br>\n",
    "Luckily, there is another option! The German government publishes shape files of all municipalities Germany. You might ask yourself now how that is going to help us in regards to neighborhoods. The shapefiles provide us with unique polygons for each municipality that we can use to filter our Facebook data. Once filtered, the amount of coordinates that need to be reverse engineered is going to be much less time and cost intensive.\n",
    "<br>\n",
    "<br>\n",
    "The shape files can be downloaded on this [website](https://gdz.bkg.bund.de/index.php/default/digitale-geodaten/verwaltungsgebiete.html) for free. For this excercise, I have used the [vg250_12-31.utm32s.shape.ebenen](https://daten.gdz.bkg.bund.de/produkte/vg/vg250_ebenen_1231/aktuell/vg250_12-31.utm32s.shape.ebenen.zip) dataset. Once you click the link it will direct you to download the respective zip file. Alternatively, you can access the needed files on the Github repository. \n",
    "<br>\n",
    "<br>\n",
    "The following code will allow you to load the shape file and convert it into a pandas data frame:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(11139, 27)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   ADE  GF  BSG           ARS       AGS       SDV_ARS          GEN    BEZ  \\\n",
       "0    6   4    1  010010000000  01001000  010010000000    Flensburg  Stadt   \n",
       "1    6   4    1  010020000000  01002000  010020000000         Kiel  Stadt   \n",
       "2    6   4    1  010030000000  01003000  010030000000       Lübeck  Stadt   \n",
       "3    6   4    1  010040000000  01004000  010040000000   Neumünster  Stadt   \n",
       "4    6   4    1  010510011011  01051011  010510011011  Brunsbüttel  Stadt   \n",
       "\n",
       "   IBZ        BEM  ... FK_S3   NUTS         ARS_0     AGS_0         WSK  \\\n",
       "0   60  kreisfrei  ...     R  DEF01  010010000000  01001000  2008-01-01   \n",
       "1   60  kreisfrei  ...     R  DEF02  010020000000  01002000  2006-01-01   \n",
       "2   60  kreisfrei  ...     R  DEF03  010030000000  01003000  2006-02-01   \n",
       "3   60  kreisfrei  ...     R  DEF04  010040000000  01004000  1970-04-26   \n",
       "4   61         --  ...     R  DEF05  010510011011  01051011  2009-01-01   \n",
       "\n",
       "           DEBKG_ID            RS        SDV_RS          RS_0  \\\n",
       "0  DEBKGDL20000E5MA  010010000000  010010000000  010010000000   \n",
       "1  DEBKGDL20000004J  010020000000  010020000000  010020000000   \n",
       "2  DEBKGDL20000DYMA  010030000000  010030000000  010030000000   \n",
       "3  DEBKGDL20000E4SA  010040000000  010040000000  010040000000   \n",
       "4  DEBKGDL20000E2IK  010510011011  010510011011  010510011011   \n",
       "\n",
       "                                              coords  \n",
       "0  [(526513.7529476011, 6075133.41194521), (52654...  \n",
       "1  [(575841.569459631, 6032148.031753651), (57586...  \n",
       "2  [(623056.1506336611, 5983746.445214357), (6231...  \n",
       "3  [(565015.6516448742, 6000637.513467715), (5651...  \n",
       "4  [(510789.9284805175, 5977425.101718364), (5109...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ADE</th>\n      <th>GF</th>\n      <th>BSG</th>\n      <th>ARS</th>\n      <th>AGS</th>\n      <th>SDV_ARS</th>\n      <th>GEN</th>\n      <th>BEZ</th>\n      <th>IBZ</th>\n      <th>BEM</th>\n      <th>...</th>\n      <th>FK_S3</th>\n      <th>NUTS</th>\n      <th>ARS_0</th>\n      <th>AGS_0</th>\n      <th>WSK</th>\n      <th>DEBKG_ID</th>\n      <th>RS</th>\n      <th>SDV_RS</th>\n      <th>RS_0</th>\n      <th>coords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>4</td>\n      <td>1</td>\n      <td>010010000000</td>\n      <td>01001000</td>\n      <td>010010000000</td>\n      <td>Flensburg</td>\n      <td>Stadt</td>\n      <td>60</td>\n      <td>kreisfrei</td>\n      <td>...</td>\n      <td>R</td>\n      <td>DEF01</td>\n      <td>010010000000</td>\n      <td>01001000</td>\n      <td>2008-01-01</td>\n      <td>DEBKGDL20000E5MA</td>\n      <td>010010000000</td>\n      <td>010010000000</td>\n      <td>010010000000</td>\n      <td>[(526513.7529476011, 6075133.41194521), (52654...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>4</td>\n      <td>1</td>\n      <td>010020000000</td>\n      <td>01002000</td>\n      <td>010020000000</td>\n      <td>Kiel</td>\n      <td>Stadt</td>\n      <td>60</td>\n      <td>kreisfrei</td>\n      <td>...</td>\n      <td>R</td>\n      <td>DEF02</td>\n      <td>010020000000</td>\n      <td>01002000</td>\n      <td>2006-01-01</td>\n      <td>DEBKGDL20000004J</td>\n      <td>010020000000</td>\n      <td>010020000000</td>\n      <td>010020000000</td>\n      <td>[(575841.569459631, 6032148.031753651), (57586...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>4</td>\n      <td>1</td>\n      <td>010030000000</td>\n      <td>01003000</td>\n      <td>010030000000</td>\n      <td>Lübeck</td>\n      <td>Stadt</td>\n      <td>60</td>\n      <td>kreisfrei</td>\n      <td>...</td>\n      <td>R</td>\n      <td>DEF03</td>\n      <td>010030000000</td>\n      <td>01003000</td>\n      <td>2006-02-01</td>\n      <td>DEBKGDL20000DYMA</td>\n      <td>010030000000</td>\n      <td>010030000000</td>\n      <td>010030000000</td>\n      <td>[(623056.1506336611, 5983746.445214357), (6231...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>4</td>\n      <td>1</td>\n      <td>010040000000</td>\n      <td>01004000</td>\n      <td>010040000000</td>\n      <td>Neumünster</td>\n      <td>Stadt</td>\n      <td>60</td>\n      <td>kreisfrei</td>\n      <td>...</td>\n      <td>R</td>\n      <td>DEF04</td>\n      <td>010040000000</td>\n      <td>01004000</td>\n      <td>1970-04-26</td>\n      <td>DEBKGDL20000E4SA</td>\n      <td>010040000000</td>\n      <td>010040000000</td>\n      <td>010040000000</td>\n      <td>[(565015.6516448742, 6000637.513467715), (5651...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>4</td>\n      <td>1</td>\n      <td>010510011011</td>\n      <td>01051011</td>\n      <td>010510011011</td>\n      <td>Brunsbüttel</td>\n      <td>Stadt</td>\n      <td>61</td>\n      <td>--</td>\n      <td>...</td>\n      <td>R</td>\n      <td>DEF05</td>\n      <td>010510011011</td>\n      <td>01051011</td>\n      <td>2009-01-01</td>\n      <td>DEBKGDL20000E2IK</td>\n      <td>010510011011</td>\n      <td>010510011011</td>\n      <td>010510011011</td>\n      <td>[(510789.9284805175, 5977425.101718364), (5109...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 27 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Import working libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Define function to read shape files\n",
    "def read_shapefile(shp_path):\n",
    "\t\"\"\"\n",
    "\tRead a shapefile into a Pandas dataframe with a 'coords' column holding\n",
    "\tthe geometry information. This uses the pyshp package.\n",
    "\t\"\"\"\n",
    "    # Import working libraries\n",
    "\timport shapefile\n",
    "\n",
    "\t# Read file, parse out the records and shapes\n",
    "\tsf = shapefile.Reader(shp_path)\n",
    "\tfields = [x[0] for x in sf.fields][1:]\n",
    "\trecords = sf.records()\n",
    "\tshps = [s.points for s in sf.shapes()]\n",
    "\n",
    "\t# Write into a dataframe\n",
    "\tdf = pd.DataFrame(columns=fields, data=records)\n",
    "\tdf = df.assign(coords=shps)\n",
    "\n",
    "    # Check out\n",
    "\treturn df\n",
    "\n",
    "# Set path\n",
    "path = r\"C:\\Users\\maurice.buettgenbach\\OneDrive - Aquila Capital Management GmbH\\Desktop\\Desktop\\Private\\IBM\\10_Capstone project\\IBM_The_battle_of_neighborhoods\\VG250_GEM.shp\"\n",
    "\n",
    "# Run function\n",
    "municipalities = read_shapefile(path)\n",
    "\n",
    "#Visualize\n",
    "print(municipalities.shape)\n",
    "neighborhood_polygons.head()"
   ]
  },
  {
   "source": [
    "### OPEN STREET MAP\n",
    "To reverse geocode our filtered coordinates, we will use the publicly available geospatial data by [OpenStreetMap](https://wiki.openstreetmap.org/wiki/Main_Page). OpenStreetMap is a non profit organization that collects geospatial data and makes in available to the public. We will use this data to reverse geocode our filtered coordinates. We can do so by using the service [Nominatim](https://wiki.openstreetmap.org/wiki/Nominatim) via the [GeoPy](https://geopy.readthedocs.io/en/stable/) library. More details to this in the library section. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### L'OSTERIA DATA\n",
    "Finally, we will use the geospatial data from the L'Osteria website to localize their existing restaurants. As the chain is rather big and it would take too long to extract the coordinates from the html by hand, we will use the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to scrape the address data from the website. We then can use Nominatim to receive their coordinates. The following code scrapes and cleans the data so that we can use it later-on:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 399.38it/s]\n",
      "\n",
      "df shape: (12, 3)\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                      STREET ZIP_CODE       CITY\n",
       "0         Gut-Dämme-Straße 1    52070     Aachen\n",
       "1        Franziskanergasse 5    92224     Amberg\n",
       "2    Pearl-S.-Buck-Straße 12    86156   Augsburg\n",
       "3   Albert-Schenavsky-Str. 2    86165   Augsburg\n",
       "4       Maximilianstrasse 83    95444   Bayreuth\n",
       "5      Hilda-Geiringer-Weg 4    10557     Berlin\n",
       "6            AltMahlsdorf 88    12623     Berlin\n",
       "7            Memhardstraße 3    10178     Berlin\n",
       "8    Mildred-Harnack-Str. 11    10243     Berlin\n",
       "9             Savignyplatz 5    10623     Berlin\n",
       "10       Obernstraße 53 - 55    33602  Bielefeld\n",
       "11  BudapesterStraße 38 - 50    10787     Berlin"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STREET</th>\n      <th>ZIP_CODE</th>\n      <th>CITY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Gut-Dämme-Straße 1</td>\n      <td>52070</td>\n      <td>Aachen</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Franziskanergasse 5</td>\n      <td>92224</td>\n      <td>Amberg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pearl-S.-Buck-Straße 12</td>\n      <td>86156</td>\n      <td>Augsburg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Albert-Schenavsky-Str. 2</td>\n      <td>86165</td>\n      <td>Augsburg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Maximilianstrasse 83</td>\n      <td>95444</td>\n      <td>Bayreuth</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Hilda-Geiringer-Weg 4</td>\n      <td>10557</td>\n      <td>Berlin</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>AltMahlsdorf 88</td>\n      <td>12623</td>\n      <td>Berlin</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Memhardstraße 3</td>\n      <td>10178</td>\n      <td>Berlin</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Mildred-Harnack-Str. 11</td>\n      <td>10243</td>\n      <td>Berlin</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Savignyplatz 5</td>\n      <td>10623</td>\n      <td>Berlin</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Obernstraße 53 - 55</td>\n      <td>33602</td>\n      <td>Bielefeld</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>BudapesterStraße 38 - 50</td>\n      <td>10787</td>\n      <td>Berlin</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "# Import working libraries\n",
    "import requests # Request handling\n",
    "from bs4 import BeautifulSoup # HTML search\n",
    "import re # String handling\n",
    "\n",
    "# Define link\n",
    "link = \"https://losteria.net/de/restaurants/view/list/?tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5Bcountry%5D=de&tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5BsearchTerm%5D=&&&tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5Btype%5D=reservations&\"\n",
    "\n",
    "# Download page\n",
    "page = requests.get(link)\n",
    "\n",
    "# Create soup object\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Define results\n",
    "results = soup.find(id='losteria-restaurants-list-wrapper')\n",
    "\n",
    "# Find elements\n",
    "page_elements = results.find_all('div', class_='address')\n",
    "\n",
    "# Create variable to store data in\n",
    "restaurants = pd.DataFrame()\n",
    "\n",
    "# Iterate through found elements\n",
    "for page_element in tqdm(page_elements):\n",
    "    # Isolate address\n",
    "    element = str(page_element)\n",
    "    # Get rid of unnecessary characters\n",
    "    element = element.replace(\"\\n\", \"\")\n",
    "    element = element.replace(\"\\r\", \"\")\n",
    "    # Get rid of spaces\n",
    "    element = element.replace(\" \", \"\")\n",
    "    # Search for address in between <div>\n",
    "    search_result = re.search(\">(.*)<\", element)\n",
    "    # Store search result in new variable\n",
    "    address = search_result.group(1)\n",
    "    # Get street\n",
    "    street = address.split(\",\")[0]\n",
    "    # Add space between street name and house number\n",
    "    street = re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", street).strip()\n",
    "    # Get ZIP code and city\n",
    "    address = address.split(\",\")[1]\n",
    "    # Isolate ZIP code\n",
    "    zip_code = re.split('(\\d+)', address)[1]\n",
    "    # Isolate city\n",
    "    city = re.split('(\\d+)', address)[2]\n",
    "    # Create temporary df\n",
    "    temp_df = pd.DataFrame()\n",
    "    # Add data to df\n",
    "    temp_df['STREET'] = [street]\n",
    "    temp_df['ZIP_CODE'] = zip_code\n",
    "    temp_df['CITY'] = city\n",
    "    # Append to variable\n",
    "    restaurants = restaurants.append(temp_df)\n",
    "# Reset index\n",
    "restaurants.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add spacer for output\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Visualize variable\n",
    "print(\"df shape:\", restaurants.shape)\n",
    "restaurants.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "WebDriverException",
     "evalue": "Message: 'geckodriver' executable needs to be in PATH. \n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_line_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             self.process = subprocess.Popen(cmd, env=self.env,\n\u001b[0m\u001b[0;32m     73\u001b[0m                                             \u001b[0mclose_fds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Windows'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    948\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1415\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1416\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1417\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-1a464a348684>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexpected_conditions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mEC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# use firefox to get page with javascript generated content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFirefox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://losteria.net/de/restaurants/view/list/?tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5Bcountry%5D=de&tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5BsearchTerm%5D=&tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5Btype%5D=open&tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5BmapView%5D=&cHash=6d126f1cfc4964f9c642d679c3674314\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mbutton\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'losteria-restaurants-loader'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\firefox\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, firefox_profile, firefox_binary, timeout, capabilities, proxy, executable_path, options, service_log_path, firefox_options, service_args, desired_capabilities, log_path, keep_alive)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mservice_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 log_path=service_log_path)\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mcapabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_capabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 raise WebDriverException(\n\u001b[0m\u001b[0;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m     83\u001b[0m                         os.path.basename(self.path), self.start_error_message)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: 'geckodriver' executable needs to be in PATH. \n"
     ]
    }
   ],
   "source": [
    "from contextlib import closing\n",
    "from selenium.webdriver import Firefox # pip install selenium\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "# use firefox to get page with javascript generated content\n",
    "with closing(Firefox()) as driver:\n",
    "    driver.get(\"https://losteria.net/de/restaurants/view/list/?tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5Bcountry%5D=de&tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5BsearchTerm%5D=&tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5Btype%5D=open&tx_losteriarestaurants_restaurantsreopening%5Bfilter%5D%5BmapView%5D=&cHash=6d126f1cfc4964f9c642d679c3674314\")\n",
    "    button = driver.find_element_by_id('losteria-restaurants-loader')\n",
    "    button.click()\n",
    "    # wait for the page to load\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "    EC.invisibility_of_element_located((By.ID, \"losteria-restaurants-loader\"))\n",
    "    )\n",
    "    # store it to string variable\n",
    "    page_source = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(page_source)\n",
    "items = soup.findAll('div', {\"class\": \"list-item\"})\n",
    "print(\"items count:\",len(items))"
   ]
  },
  {
   "source": [
    "## THE LIBRARIES <a name=\"THE_LIBRARIES\" ></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# URL handling\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "# Web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Geo data\n",
    "from turfpy.measurement import boolean_point_in_polygon\n",
    "from geojson import Point, Polygon, Feature\n",
    "\n",
    "# Visualization\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check out\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point = Feature(geometry=Point((-46.6318, -23.5523)))\n",
    "# polygon = Polygon(\n",
    "#     [\n",
    "#         [\n",
    "#             (-46.653, -23.543),\n",
    "#             (-46.634, -23.5346),\n",
    "#             (-46.613, -23.543),\n",
    "#             (-46.614, -23.559),\n",
    "#             (-46.631, -23.567),\n",
    "#             (-46.653, -23.560),\n",
    "#             (-46.653, -23.543),\n",
    "#         ]\n",
    "#     ]\n",
    "# )\n",
    "# boolean_point_in_polygon(point, polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1000/1000 [08:19<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create temporary variable\n",
    "df = population.head(1000)\n",
    "\n",
    "# Create empty list \n",
    "addresses = []\n",
    "\n",
    "# Create counter\n",
    "counter = 0\n",
    "\n",
    "# Set geolocator\n",
    "geolocator = Nominatim(user_agent=\"german_italian_restaurant\")\n",
    "\n",
    "# Loop through df and get addresses for coordinates\n",
    "for i in tqdm(range(len(df))):\n",
    "    # Extract coordinates\n",
    "    lat = str(df.iloc[i, 0:1].to_numpy()[0])\n",
    "    lng = str(df.iloc[i, 1:2].to_numpy()[0])\n",
    "    # Create string from coordinates\n",
    "    coordinates = str(lat + \", \" + lng)\n",
    "    # Get location with geopy\n",
    "    location = geolocator.reverse(coordinates)\n",
    "    # Store in variable\n",
    "    addresses.append(location.address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 464.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create address data\n",
    "address_data = pd.DataFrame()\n",
    "\n",
    "# Loop through addresses and append them to adress df\n",
    "for i in tqdm(range(len(addresses))):\n",
    "    # Split strings\n",
    "    temp = addresses[i].split(\",\")\n",
    "    # Convert to df\n",
    "    df = pd.DataFrame(temp)\n",
    "    # Transpose\n",
    "    df = pd.DataFrame.transpose(df)\n",
    "    # Concatenate\n",
    "    frames = [df, address_data]\n",
    "    address_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 44%|████▍     | 11/25 [00:00<00:00, 89.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through address df and check for inconsistency\n",
    "for column in tqdm(address_data):\n",
    "    # Check for NaN values\n",
    "    if address_data[column].isnull().values.any():\n",
    "        # If NaN detected, check each row\n",
    "        for i in range(len(address_data)):\n",
    "            # Check for NaN in cell\n",
    "            if pd.isna(address_data.iloc[i, column]) == True:\n",
    "                # If NaN in cell, shift row\n",
    "                address_data.iloc[i] = address_data.iloc[i].shift(1)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25, 11)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    0                    1                    2   \\\n",
       "0  NaN                  NaN                  NaN   \n",
       "0  NaN  Trampolin oder Pool   Moosbronner Straße   \n",
       "0  NaN                  145        Gaistalstraße   \n",
       "0  NaN                   21       Im Wiesengrund   \n",
       "0  NaN                  NaN                   28   \n",
       "\n",
       "                                  3           4               5   \\\n",
       "0  Bienenzüchterverein Bad Herrenalb   Herrenalb   Bad Herrenalb   \n",
       "0                             Althof    Bernbach   Bad Herrenalb   \n",
       "0                    Unteres Gaistal   Herrenalb   Bad Herrenalb   \n",
       "0                    Unteres Gaistal   Herrenalb   Bad Herrenalb   \n",
       "0                    Wallfahrtstraße     Neusatz   Bad Herrenalb   \n",
       "\n",
       "                                       6                7   \\\n",
       "0   Verwaltungsgemeinschaft Bad Herrenalb   Landkreis Calw   \n",
       "0   Verwaltungsgemeinschaft Bad Herrenalb   Landkreis Calw   \n",
       "0   Verwaltungsgemeinschaft Bad Herrenalb   Landkreis Calw   \n",
       "0   Verwaltungsgemeinschaft Bad Herrenalb   Landkreis Calw   \n",
       "0   Verwaltungsgemeinschaft Bad Herrenalb   Landkreis Calw   \n",
       "\n",
       "                   8       9             10  \n",
       "0   Baden-Württemberg   76332   Deutschland  \n",
       "0   Baden-Württemberg   76332   Deutschland  \n",
       "0   Baden-Württemberg   76332   Deutschland  \n",
       "0   Baden-Württemberg   76332   Deutschland  \n",
       "0   Baden-Württemberg   76332   Deutschland  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Bienenzüchterverein Bad Herrenalb</td>\n      <td>Herrenalb</td>\n      <td>Bad Herrenalb</td>\n      <td>Verwaltungsgemeinschaft Bad Herrenalb</td>\n      <td>Landkreis Calw</td>\n      <td>Baden-Württemberg</td>\n      <td>76332</td>\n      <td>Deutschland</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>Trampolin oder Pool</td>\n      <td>Moosbronner Straße</td>\n      <td>Althof</td>\n      <td>Bernbach</td>\n      <td>Bad Herrenalb</td>\n      <td>Verwaltungsgemeinschaft Bad Herrenalb</td>\n      <td>Landkreis Calw</td>\n      <td>Baden-Württemberg</td>\n      <td>76332</td>\n      <td>Deutschland</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>145</td>\n      <td>Gaistalstraße</td>\n      <td>Unteres Gaistal</td>\n      <td>Herrenalb</td>\n      <td>Bad Herrenalb</td>\n      <td>Verwaltungsgemeinschaft Bad Herrenalb</td>\n      <td>Landkreis Calw</td>\n      <td>Baden-Württemberg</td>\n      <td>76332</td>\n      <td>Deutschland</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>21</td>\n      <td>Im Wiesengrund</td>\n      <td>Unteres Gaistal</td>\n      <td>Herrenalb</td>\n      <td>Bad Herrenalb</td>\n      <td>Verwaltungsgemeinschaft Bad Herrenalb</td>\n      <td>Landkreis Calw</td>\n      <td>Baden-Württemberg</td>\n      <td>76332</td>\n      <td>Deutschland</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>28</td>\n      <td>Wallfahrtstraße</td>\n      <td>Neusatz</td>\n      <td>Bad Herrenalb</td>\n      <td>Verwaltungsgemeinschaft Bad Herrenalb</td>\n      <td>Landkreis Calw</td>\n      <td>Baden-Württemberg</td>\n      <td>76332</td>\n      <td>Deutschland</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "print(address_data.shape)\n",
    "address_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column names\n",
    "columns = [\n",
    "    'STREET_NUMBER',\n",
    "    'STREET',\n",
    "    'NEIGHBORHOOD',\n",
    "    'CITY'\n",
    "    ]"
   ]
  }
 ]
}